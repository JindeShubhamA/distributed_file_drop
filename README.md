# 2020_group_12_s3993914_s4091221_s4199456

## Problem Statement
There is a requirement for a system which allows clients to connect to a pool on a hosted system to upload files and periodically receive async updates after some time. Multiple pools can be running simultaneously that require some Identity and Access Management. A client pool may consist of 2 to thousands of client applications. Each client application performs some task that produces a file that is required by a hosted server. Each client in the pool generates these files asynchronously (some clients may even be listeners and provide no updates) and sends them to our project system through an API for processing, there is a user defined update frequency from minutes to days required. Some clients may not be able to send updates immediately and will cache local updates until a batched update can be sent with multiple timestamps contained.

The file size(s) being sent are 100’s of Kbs to 10’s of Mbs. Although sending updates asynchronously, it is required that batches from similar timestamps are grouped, this can be through the range on the timestamp, or otherwise. A server, external to this system, will request a group's files at a given trigger event. The external server will then perform some operation for roughly 2 minutes. On completion the external server will return to our project system a single file intended to be shared back down to the entire client pool.

## Architecture

![Architecture7](Architecture/Architecture-7.png)

The above image summarizes the architecture followed in the project. The whole system is designed with microservice architecture. Every module is an independent microservice. Different microservices interact with each other via synchronous or ansynchronous mechanisms, as per the need. Let us look at each module in detail.

### Front-end Client

This module is a dummy web client which provides the following features :
* Create a new user
* Login
* Logout
* Upload a new file
* Allow admin to view files for all users

It calls the Authentication Service APIs to create a new user, login and logout.

It connects to the socket gateway to allow the user to upload file, and to allow them to view their previously uploaded files.

### Socket Gateway

It provides an interface for asynchronous communication between front-end client and the backend servers. It exposes following web-socket connection APIs :
* Initiate Connection : `connect`
* Start Uploading a file : `start-transfer`
* File Upload : `write-chunk`
* Completing the Upload : `complete-upload`
* View user history : `get-history`
* Keeping Connection Alive : `alive`

Initiate Connection API allows user to connect to the back-end system. User has to provide their `access_token`. Connection is established only if this `access_token` is valid. If it is not, user has to login and get a new `access_token`.

Start Uploading a file lets the user to select the file to be uploaded. If it is one of the forbidden types, upload is rejected. Else, a unique file_id is created to store this file in a temporary storage. It also records user details and receives file metadata from the user.

File Upload actually uplaods the file against the unique `file_id` created in the previous API. This is the actual movement of the file from client to server.

Completing the upload determines when the file is completely uplaoded. It verifies it against the user details and metadata recorded while starting the upload. If verification proceeds successfully, it internally calls the `File Upload` API of the file uploader service to allow further processing.

View User History internally calls the `View User History` API of the File Service Manager to fetch details about all the files uploaded by the user.

Keeping connection alive provides mechanism for the client to keep the connection alive even if they are not using for its actual feature.

In addition, the gateway also runs a queue consumer in a parallel thread. This consumer consumes messages from the user notification queues and publishes them to the client via the established socket connection. This mechanism is implemented to asynchronously update the user about the status of the file they attempted to upload.

### Authentication Service
This module is reponsible for authorising users to use the whole system. It exposes the following HTTP REST APIs:
* Create new user : `POST /auth/signup`
* Login existing user : `PUT /auth/login/<string:user_name>`
* Logout user : `PUT /auth/logout/<string:user_name>`
* Authenticate user : `GET /auth/validate`

A user entitiy is defined by :
* User ID : System Generated [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier)
* User Name : Defined by the user during sign up. Needs to be unique.
* Role : `admin` or `user`

Each user is assigned an `access_token` generated by the system at the time of user creation. This is used to authenticate the user for further requests. A user, immediately after creation is logged in by default.

Access Token is valid for a logged in user. If the user logs out, the access token becomes invalid. User needs to login again to generate a fresh access token in this case.

Access Token information in both the above stated scenarios is shared with the user via [JSON Web Token](https://jwt.io/)

Authentication API checks if the given `access_token` is valid for the given `user_id`. It returns error if the user is not authenticated or not authorised.

User data is maintained in relational form in a PostgreSQL Database.

We do not have password based authentication yet, because this module is not the focus point of the project. A simple to use authentication system is created, but it provides a window for future development into complex authentication mechanism.

### File Uploader Service

This module is responsible for receiving file from the user, validating it, and caching it until it is stored in a permanent storage. It exposes the following HTTP REST APIs:
* Upload File : `POST /file/upload`
* Update File Status : `PUT /file/update/status`

Upload File API recieves the file along with user details and some metadata. It validates the file against `EMPTY_FILE` checks. It generates a `file_id` for this file, and then stores the file, as well as the associated details in cache. It then publishes a queue event with the file details and user details for facilitating its permanenet storage. It also publishes another queue event for the notifying the user (and the admin as well) that file has been successfully cached.

Update File Status API is hit by the File Service Manager to update the status of the file : `uploaded_successfully` or `upload_failed`.

In case of `uploaded_successfully`, file is deleted from the cache, its index is updated with its successfully uplaoded status, and an event is published in the queue to notify the user (and the admin as well) the file has been uploaded successfully.

However, in case of `upload_failed`, an event to the file upload queue is published again for facilitating the retry of upload flow. Index of the file in the cache is updated to have information about the retry. A file is retried for uplaod two times (If counting the original uplaod attempt, we have total 3 attempts for a single file). If upload is still not successful, it deletes the file from the cache. Index of the file is marked as failed to upload, and an event in the user notification queue is published to notify the user of the failure.

We use Redis for in-memory caching requirements, and RabbitMQ for event queueing mechanisms.

### File Service Manager

This module is responsible for storing the cached files in permanent storage, and providing an interface to view and download them.

It consumes the file upload queue events which contain the file details and associated user details. It first tries to fetch the file from the cache, and then uploads it in a permanent storage database. It then creates an index entry for the file in the database. This entry is marked against the user who owns the file. Once all of this is done, it informs the File Uploader service about the status of the file via the `Update File Status` API.

Alongside this queue consumer, this module also runs a server instance. It exposes the following HTTP REST API :
* View User History `GET /client/history/<client_id>`

The View User History API fetches the indexing details of all the files uploaded by the given user.

Here, we use GridFS as permanent File Storage and MongoDB for storing the file index information. Caching and queueing mechaninsms are the same as instantiated by File Uploader Service.


## Dependencies

The project uses the following technological stack:

* Python 3.8
* Redis
* RabbitMQ
* PostgreSQL
* GridFS
* MongoDB
* Kibana
* Docker
* Kubernetes

## Running the program



## Changelog
React App currently getting a JWT token generated from a static endpoint working only for 'test' user

https://zgadzaj.com/development/docker/docker-compose/containers/rabbitmq

```bash
use kompose convert
cp files folder
cd folder
kubectl apply -R -f .
```
